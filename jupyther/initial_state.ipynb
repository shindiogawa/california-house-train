{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b2c1f1e-8d97-437e-b4ec-271953c04958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# criar spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config('spark.jars.packages','org.postgresql:postgresql:42.2.20') \\\n",
    "    .appName('spark etl - cht') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "POSTGRES_DB_IP = '172.19.0.1'\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/dadosmaistodos/challenge-data-engineering/main/california_housing_train.csv\");\n",
    "\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "spark_df.printSchema()\n",
    "\n",
    "\n",
    "spark_df.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", f\"jdbc:postgresql://{POSTGRES_DB_IP}:5432/cht\") \\\n",
    "  .option(\"dbtable\", \"cht_raw\") \\\n",
    "  .option(\"user\", \"postgres\") \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .option(\"password\", \"postgres\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3d62eae-5b9a-43b1-bc53-f4c80dac1554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cht_id: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      "\n",
      "+--------------------+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|              cht_id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
      "+--------------------+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|2908c11b-007c-483...|  -118.33|    34.2|              43.0|     2322.0|         418.0|    1106.0|     433.0|       4.3631|          284600.0|\n",
      "|aa21f291-df84-4d7...|  -118.33|    34.2|              43.0|     1325.0|         254.0|     613.0|     248.0|       3.6071|          289000.0|\n",
      "|546de930-e054-48e...|  -118.33|    34.2|              23.0|     7179.0|        1985.0|    4757.0|    1924.0|       3.1051|          206500.0|\n",
      "|94ff4c44-a174-440...|  -118.33|   34.19|              46.0|     2115.0|         463.0|    1133.0|     439.0|       3.7344|          222000.0|\n",
      "|a185fa23-3093-42f...|  -118.33|   34.19|              45.0|     1505.0|         347.0|     799.0|     319.0|        3.138|          217000.0|\n",
      "|768ef6ec-3a8b-450...|  -118.33|   34.18|              49.0|     1969.0|         377.0|     977.0|     367.0|       3.8462|          231300.0|\n",
      "|da56c982-3234-4f1...|  -118.33|   34.18|              48.0|     2122.0|         385.0|     926.0|     362.0|       5.6975|          231400.0|\n",
      "|76c4b06a-5e28-486...|  -118.33|   34.18|              45.0|     1552.0|         315.0|     785.0|     316.0|       3.7411|          235500.0|\n",
      "|9bc9799c-17e9-479...|  -118.33|   34.17|              48.0|     2584.0|         483.0|    1118.0|     459.0|       4.2396|          245100.0|\n",
      "|72ff16ea-d3ae-4b8...|  -118.33|   34.17|              44.0|     1934.0|         375.0|     750.0|     365.0|        2.473|          251800.0|\n",
      "|6ffed70f-31b1-469...|  -118.33|   34.16|              44.0|     2705.0|         649.0|    1676.0|     654.0|       3.4286|          247900.0|\n",
      "|97d9d15a-c771-4ec...|  -118.33|   34.16|              37.0|     2381.0|         575.0|    1235.0|     499.0|       3.7941|          247800.0|\n",
      "|fa8bf364-74be-484...|  -118.33|   34.16|              23.0|     1359.0|         428.0|     770.0|     380.0|       3.4016|          234600.0|\n",
      "|97cfe843-6b24-439...|  -118.33|   34.15|              44.0|     1321.0|         303.0|     471.0|     301.0|       4.2679|          331800.0|\n",
      "|c77affb6-1fe7-41b...|  -118.33|   34.15|              39.0|      493.0|         168.0|     259.0|     138.0|       2.3667|           17500.0|\n",
      "|ff758806-9b19-40e...|  -118.33|   34.12|              23.0|     1894.0|         416.0|     769.0|     392.0|       6.0352|          500001.0|\n",
      "|558d082a-e76c-4d1...|  -118.33|   34.11|              38.0|     3495.0|        1100.0|    1939.0|     994.0|       2.2148|          438300.0|\n",
      "|e8b8ec7f-1342-439...|  -118.33|    34.1|              48.0|     1116.0|         524.0|    1610.0|     483.0|        1.625|          237500.0|\n",
      "|69c485fd-27ca-4de...|  -118.33|    34.1|              43.0|     2732.0|        1646.0|    3049.0|    1429.0|       1.3157|          333300.0|\n",
      "|e7cc37bf-1a01-4ab...|  -118.33|    34.1|              29.0|      732.0|         288.0|     691.0|     278.0|       2.1866|          250000.0|\n",
      "+--------------------+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cht_raw = spark.read \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", f\"jdbc:postgresql://{POSTGRES_DB_IP}:5432/cht\") \\\n",
    "  .option(\"dbtable\", \"stg.stg_cht_raw\") \\\n",
    "  .option(\"user\", \"postgres\") \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .option(\"password\", \"postgres\")\\\n",
    "  .load()\n",
    "\n",
    "cht_raw.printSchema()\n",
    "\n",
    "cht_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e1baf6df-f834-4c80-8875-470f3e58c4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                name|     desvio_padrao|\n",
      "+--------------------+------------------+\n",
      "|median_house_valu...|115983.76438720878|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window as W\n",
    "from pyspark.sql import functions as F\n",
    "cht_raw.createOrReplaceTempView(\"cht_view_in_memory\")\n",
    "\n",
    "df_hct_unique = spark.sql(\"\"\"\n",
    "                  select \n",
    "                  *\n",
    "                  from cht_view_in_memory\n",
    "                  \"\"\")\n",
    "\n",
    "\n",
    "# df_cht_dp = spark.sql(\"\"\"\n",
    "#                   select\n",
    "#                       'longitude' as name,\n",
    "#                       longitude as value\n",
    "#                   from cht_view_in_memory\n",
    "#                   union all\n",
    "#                   select\n",
    "#                       'latitude' as name,\n",
    "#                       latitude as value\n",
    "#                   from cht_view_in_memory\n",
    "#                   union all\n",
    "#                   select\n",
    "#                       'housing_median_age' as name,\n",
    "#                       housing_median_age as value\n",
    "#                   from cht_view_in_memory\n",
    "#                   union all\n",
    "#                   select\n",
    "#                       'total_rooms' as name,\n",
    "#                       total_rooms as value\n",
    "#                   from cht_view_in_memory\n",
    "#                   union all\n",
    "#                   select\n",
    "#                       'total_bedrooms' as name,\n",
    "#                       total_bedrooms as value\n",
    "#                   from cht_view_in_memory\n",
    "#                   union all\n",
    "#                   select\n",
    "#                       'population' as name,\n",
    "#                       population as value\n",
    "#                   from cht_view_in_memory\n",
    "#                   union all\n",
    "#                   select\n",
    "#                       'households' as name,\n",
    "#                       households as value\n",
    "#                   from cht_view_in_memory\n",
    "#                   union all\n",
    "#                   select\n",
    "#                       'median_income' as name,\n",
    "#                       median_income as value\n",
    "#                   from cht_view_in_memory\n",
    "#                   union all\n",
    "#                   select\n",
    "#                       'median_house_value' as name,\n",
    "#                       median_house_value as value\n",
    "#                   from cht_view_in_memory\n",
    "#                   \"\"\")\n",
    "# df_cht_dp.createOrReplaceTempView(\"cht_view_all_metrics\")\n",
    "\n",
    "# df_cht_metrics = spark.sql(\"\"\"\n",
    "#                   select distinct\n",
    "#                   *\n",
    "#                   from cht_view_all_metrics\n",
    "#                   \"\"\").show()\n",
    "# df_dp = df_cht_metrics.select(stddev(col('value')).over(W.partitionBy('name')).alias('desvio_padrao')).show()\n",
    "# df_cht_metrics.withColumn('desvio_padrao', stddev(col('value')).over(W.partitionBy('name'))).show()\n",
    "\n",
    "df = df_hct_unique.select(\n",
    "    stddev(col('longitude')).alias('longitude_dp'),\n",
    "    stddev(col('latitude')).alias('latitude_dp'),\n",
    "    stddev(col('housing_median_age')).alias('housing_median_age_dp'),\n",
    "    stddev(col('total_rooms')).alias('total_rooms_dp'),\n",
    "    stddev(col('total_bedrooms')).alias('total_bedrooms_dp'),\n",
    "    stddev(col('population')).alias('population_dp'),\n",
    "    stddev(col('households')).alias('households_dp'),\n",
    "    stddev(col('median_income')).alias('median_income_dp'),\n",
    "    stddev(col('median_house_value')).alias('median_house_value_dp')\n",
    ")\n",
    "\n",
    "df.createOrReplaceTempView(\"cht_dp_view_in_memory\")\n",
    "\n",
    "df_cht_dp = spark.sql(\"\"\"\n",
    "                  select \n",
    "                      'longitude_dp' as name,\n",
    "                      longitude_dp as desvio_padrao\n",
    "                  from cht_dp_view_in_memory\n",
    "                  union \n",
    "                  select \n",
    "                      'latitude_dp' as name,\n",
    "                      latitude_dp as desvio_padrao\n",
    "                  from cht_dp_view_in_memory\n",
    "                  union\n",
    "                  select \n",
    "                      'housing_median_age_dp' as name,\n",
    "                      housing_median_age_dp as desvio_padrao\n",
    "                  from cht_dp_view_in_memory\n",
    "                  union\n",
    "                  select \n",
    "                      'total_rooms_dp' as name,\n",
    "                      total_rooms_dp as desvio_padrao\n",
    "                  from cht_dp_view_in_memory\n",
    "                  union\n",
    "                  select \n",
    "                      'total_bedrooms_dp' as name,\n",
    "                      total_bedrooms_dp as desvio_padrao\n",
    "                  from cht_dp_view_in_memory\n",
    "                  union\n",
    "                  select \n",
    "                      'population_dp' as name,\n",
    "                      population_dp as desvio_padrao\n",
    "                  from cht_dp_view_in_memory\n",
    "                  union\n",
    "                  select \n",
    "                      'households_dp' as name,\n",
    "                      households_dp as desvio_padrao\n",
    "                  from cht_dp_view_in_memory\n",
    "                  union\n",
    "                  select \n",
    "                      'median_income_dp' as name,\n",
    "                      median_income_dp as desvio_padrao\n",
    "                  from cht_dp_view_in_memory\n",
    "                  union\n",
    "                  select \n",
    "                      'median_house_value_dp' as name,\n",
    "                      median_house_value_dp as desvio_padrao\n",
    "                  from cht_dp_view_in_memory\n",
    "                  \"\"\")\n",
    "df_cht_dp.createOrReplaceTempView(\"cht_view_all_dp_2\")\n",
    "\n",
    "df_all_dp = spark.sql(\"\"\"\n",
    "                        select name, desvio_padrao\n",
    "                        from cht_view_all_dp_2\n",
    "                        where desvio_padrao = (select max(desvio_padrao) from cht_view_all_dp)\n",
    "                    \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "eedd48e3-ebd4-4c4e-9ff8-21467cf35eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|              name|   value|\n",
      "+------------------+--------+\n",
      "|median_house_value|500001.0|\n",
      "|         longitude| -124.35|\n",
      "+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cht_dp = spark.sql(\"\"\"\n",
    "                  select \n",
    "                      'longitude' as name,\n",
    "                      longitude as value\n",
    "                  from cht_view_in_memory\n",
    "                  union \n",
    "                  select \n",
    "                      'latitude' as name,\n",
    "                      latitude as value\n",
    "                  from cht_view_in_memory\n",
    "                  union\n",
    "                  select \n",
    "                      'housing_median_age' as name,\n",
    "                      housing_median_age as value\n",
    "                  from cht_view_in_memory\n",
    "                  union\n",
    "                  select \n",
    "                      'total_rooms' as name,\n",
    "                      total_rooms as value\n",
    "                  from cht_view_in_memory\n",
    "                  union\n",
    "                  select \n",
    "                      'total_bedrooms' as name,\n",
    "                      total_bedrooms as value\n",
    "                  from cht_view_in_memory\n",
    "                  union\n",
    "                  select \n",
    "                      'population' as name,\n",
    "                      population as value\n",
    "                  from cht_view_in_memory\n",
    "                  union\n",
    "                  select \n",
    "                      'households' as name,\n",
    "                      households as value\n",
    "                  from cht_view_in_memory\n",
    "                  union\n",
    "                  select \n",
    "                      'median_income' as name,\n",
    "                      median_income as value\n",
    "                  from cht_view_in_memory\n",
    "                  union\n",
    "                  select \n",
    "                      'median_house_value' as name,\n",
    "                      median_house_value as value\n",
    "                  from cht_view_in_memory\n",
    "                  \"\"\")\n",
    "df_cht_dp.createOrReplaceTempView(\"cht_view_all_metrics\")\n",
    "\n",
    "df_all_metrics = spark.sql(\"\"\"\n",
    "                            select name, value\n",
    "                            from cht_view_all_metrics\n",
    "                            where value = (select max(value) from cht_view_all_metrics)\n",
    "                            union\n",
    "                            select name, value\n",
    "                            from cht_view_all_metrics\n",
    "                            where value = (select min(value) from cht_view_all_metrics)\n",
    "                           \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1e6679fc-9bd6-43fa-aa0e-9d8b80b95fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+-----+\n",
      "|housing_median_age|    hma_cat| c_ns|\n",
      "+------------------+-----------+-----+\n",
      "|              50.0|   acima_37|  sul|\n",
      "|              18.0|     ate_29|norte|\n",
      "|              50.0|   acima_37|norte|\n",
      "|              43.0|   acima_37|norte|\n",
      "|              17.0|de_0_ate_18|  sul|\n",
      "|              21.0|     ate_29|norte|\n",
      "|              24.0|     ate_29|norte|\n",
      "|              38.0|   acima_37|  sul|\n",
      "|              27.0|     ate_29|  sul|\n",
      "|               9.0|de_0_ate_18|norte|\n",
      "|              22.0|     ate_29|  sul|\n",
      "|              11.0|de_0_ate_18|  sul|\n",
      "|              46.0|   acima_37|  sul|\n",
      "|              34.0|     ate_37|norte|\n",
      "|              23.0|     ate_29|norte|\n",
      "|              20.0|     ate_29|norte|\n",
      "|              29.0|     ate_37|norte|\n",
      "|              11.0|de_0_ate_18|norte|\n",
      "|              16.0|de_0_ate_18|norte|\n",
      "|              26.0|     ate_29|  sul|\n",
      "+------------------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cht_math = spark.read \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", f\"jdbc:postgresql://{POSTGRES_DB_IP}:5432/cht\") \\\n",
    "  .option(\"dbtable\", \"edw.cht_math\") \\\n",
    "  .option(\"user\", \"postgres\") \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .option(\"password\", \"postgres\")\\\n",
    "  .load()\n",
    "\n",
    "cht_math.createOrReplaceTempView(\"cht_math_in_memory\")\n",
    "\n",
    "cht_math_result = spark.sql(\"\"\"\n",
    "                  select \n",
    "                  distinct housing_median_age, hma_cat, c_ns\n",
    "                  from cht_math_in_memory\n",
    "                  \"\"\")\n",
    "cht_math_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "428d3f69-b6d2-4d73-9fa9-ec6cbf74f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+--------+-----------------+\n",
      "|              cht_id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|     age|california_region|\n",
      "+--------------------+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+--------+-----------------+\n",
      "|7a9128a1-d6ff-44f...|  -118.33|    34.2|              43.0|     2322.0|         418.0|    1106.0|     433.0|       4.3631|          284600.0|acima_37|              sul|\n",
      "|1b721a65-4372-4c3...|  -118.33|    34.2|              43.0|     1325.0|         254.0|     613.0|     248.0|       3.6071|          289000.0|acima_37|              sul|\n",
      "|6d78a473-1857-452...|  -118.33|    34.2|              23.0|     7179.0|        1985.0|    4757.0|    1924.0|       3.1051|          206500.0|  ate_29|              sul|\n",
      "|1ad309c4-8589-44e...|  -118.33|   34.19|              46.0|     2115.0|         463.0|    1133.0|     439.0|       3.7344|          222000.0|acima_37|              sul|\n",
      "|437df34d-773e-4e2...|  -118.33|   34.19|              45.0|     1505.0|         347.0|     799.0|     319.0|        3.138|          217000.0|acima_37|              sul|\n",
      "|e2a33eac-5161-442...|  -118.33|   34.18|              49.0|     1969.0|         377.0|     977.0|     367.0|       3.8462|          231300.0|acima_37|              sul|\n",
      "|effe47e7-940d-439...|  -118.33|   34.18|              48.0|     2122.0|         385.0|     926.0|     362.0|       5.6975|          231400.0|acima_37|              sul|\n",
      "|5c98268c-0aea-476...|  -118.33|   34.18|              45.0|     1552.0|         315.0|     785.0|     316.0|       3.7411|          235500.0|acima_37|              sul|\n",
      "|948a1eb9-947b-40e...|  -118.33|   34.17|              48.0|     2584.0|         483.0|    1118.0|     459.0|       4.2396|          245100.0|acima_37|              sul|\n",
      "|453111c3-3c20-425...|  -118.33|   34.17|              44.0|     1934.0|         375.0|     750.0|     365.0|        2.473|          251800.0|acima_37|              sul|\n",
      "|1e62af07-d410-45d...|  -118.33|   34.16|              44.0|     2705.0|         649.0|    1676.0|     654.0|       3.4286|          247900.0|acima_37|              sul|\n",
      "|13d23d58-c639-411...|  -118.33|   34.16|              37.0|     2381.0|         575.0|    1235.0|     499.0|       3.7941|          247800.0|acima_37|              sul|\n",
      "|ef777f3c-8130-4f5...|  -118.33|   34.16|              23.0|     1359.0|         428.0|     770.0|     380.0|       3.4016|          234600.0|  ate_29|              sul|\n",
      "|d566193d-da05-458...|  -118.33|   34.15|              44.0|     1321.0|         303.0|     471.0|     301.0|       4.2679|          331800.0|acima_37|              sul|\n",
      "|89e38a58-215f-4fc...|  -118.33|   34.15|              39.0|      493.0|         168.0|     259.0|     138.0|       2.3667|           17500.0|acima_37|              sul|\n",
      "|82694625-2ba0-489...|  -118.33|   34.12|              23.0|     1894.0|         416.0|     769.0|     392.0|       6.0352|          500001.0|  ate_29|              sul|\n",
      "|c8f08a90-44b9-49b...|  -118.33|   34.11|              38.0|     3495.0|        1100.0|    1939.0|     994.0|       2.2148|          438300.0|acima_37|              sul|\n",
      "|bd6fe62b-8125-47c...|  -118.33|    34.1|              48.0|     1116.0|         524.0|    1610.0|     483.0|        1.625|          237500.0|acima_37|              sul|\n",
      "|e30d7de9-bc41-4eb...|  -118.33|    34.1|              43.0|     2732.0|        1646.0|    3049.0|    1429.0|       1.3157|          333300.0|acima_37|              sul|\n",
      "|be47a0b7-8112-4d4...|  -118.33|    34.1|              29.0|      732.0|         288.0|     691.0|     278.0|       2.1866|          250000.0|  ate_37|              sul|\n",
      "+--------------------+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cht_math_result_renamed = spark.sql(\"\"\"\n",
    "                  select \n",
    "                  *\n",
    "                  from cht_math_in_memory\n",
    "                  \"\"\")\n",
    "cht_math_result_renamed = cht_math_result_renamed.withColumnRenamed('hma_cat','age')\\\n",
    "               .withColumnRenamed('c_ns','california_region')\n",
    "\n",
    "cht_math_result_renamed.show()\n",
    "\n",
    "\n",
    "cht_math_result_renamed.createOrReplaceTempView(\"cht_math_renamed_in_memory\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "74edf3a7-4ab3-4a06-bee5-46371a014edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- california_region: string (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      "\n",
      "+--------+-----------------+-----------+--------------+----------+----------+------------------+\n",
      "|     age|california_region|total_rooms|total_bedrooms|population|households|median_house_value|\n",
      "+--------+-----------------+-----------+--------------+----------+----------+------------------+\n",
      "|acima_37|              sul|     2322.0|         418.0|    1106.0|     433.0|          284600.0|\n",
      "|acima_37|              sul|     1325.0|         254.0|     613.0|     248.0|          289000.0|\n",
      "|  ate_29|              sul|     7179.0|        1985.0|    4757.0|    1924.0|          206500.0|\n",
      "|acima_37|              sul|     2115.0|         463.0|    1133.0|     439.0|          222000.0|\n",
      "|acima_37|              sul|     1505.0|         347.0|     799.0|     319.0|          217000.0|\n",
      "|acima_37|              sul|     1969.0|         377.0|     977.0|     367.0|          231300.0|\n",
      "|acima_37|              sul|     2122.0|         385.0|     926.0|     362.0|          231400.0|\n",
      "|acima_37|              sul|     1552.0|         315.0|     785.0|     316.0|          235500.0|\n",
      "|acima_37|              sul|     2584.0|         483.0|    1118.0|     459.0|          245100.0|\n",
      "|acima_37|              sul|     1934.0|         375.0|     750.0|     365.0|          251800.0|\n",
      "|acima_37|              sul|     2705.0|         649.0|    1676.0|     654.0|          247900.0|\n",
      "|acima_37|              sul|     2381.0|         575.0|    1235.0|     499.0|          247800.0|\n",
      "|  ate_29|              sul|     1359.0|         428.0|     770.0|     380.0|          234600.0|\n",
      "|acima_37|              sul|     1321.0|         303.0|     471.0|     301.0|          331800.0|\n",
      "|acima_37|              sul|      493.0|         168.0|     259.0|     138.0|           17500.0|\n",
      "|  ate_29|              sul|     1894.0|         416.0|     769.0|     392.0|          500001.0|\n",
      "|acima_37|              sul|     3495.0|        1100.0|    1939.0|     994.0|          438300.0|\n",
      "|acima_37|              sul|     1116.0|         524.0|    1610.0|     483.0|          237500.0|\n",
      "|acima_37|              sul|     2732.0|        1646.0|    3049.0|    1429.0|          333300.0|\n",
      "|  ate_37|              sul|      732.0|         288.0|     691.0|     278.0|          250000.0|\n",
      "+--------+-----------------+-----------+--------------+----------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cht_math_result_renamed = spark.sql(\"\"\"\n",
    "                  select \n",
    "                      age,\n",
    "                      california_region,\n",
    "                      total_rooms,\n",
    "                      total_bedrooms,\n",
    "                      population,\n",
    "                      households,\n",
    "                      median_house_value\n",
    "                  from cht_math_renamed_in_memory\n",
    "                  \"\"\")\n",
    "\n",
    "cht_math_result_renamed.printSchema()\n",
    "\n",
    "cht_math_result_renamed.write.mode(\"overwrite\").parquet('./export1.parquet')\n",
    "\n",
    "spark.read.parquet('./export1.parquet').show()\n",
    "\n",
    "# age\tstring\n",
    "# california_region\tstring\n",
    "# total_rooms\tdouble\n",
    "# total_bedrooms\tdouble\n",
    "# population\tdouble\n",
    "# households\tdouble\n",
    "# median_house_value\tdouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4def0fe6-e17c-48cd-abb8-548296cbb7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- california_region: string (nullable = true)\n",
      " |-- s_population: double (nullable = true)\n",
      " |-- m_median_house_value: double (nullable = true)\n",
      "\n",
      "+-----------+-----------------+------------+--------------------+\n",
      "|        age|california_region|s_population|m_median_house_value|\n",
      "+-----------+-----------------+------------+--------------------+\n",
      "|   acima_37|              sul|   2519076.0|   227694.8277657267|\n",
      "|     ate_29|              sul|   3905630.0|  220571.65846153846|\n",
      "|   acima_37|            norte|   2114160.0|  217732.95624136343|\n",
      "|     ate_37|              sul|   3435282.0|  212266.99328593997|\n",
      "|de_0_ate_18|              sul|   4157987.0|  209407.56504269212|\n",
      "|     ate_37|            norte|   1792950.0|   195766.6032154341|\n",
      "|     ate_29|            norte|   3107411.0|  188724.02380952382|\n",
      "|de_0_ate_18|            norte|   3270261.0|  177826.69768637532|\n",
      "+-----------+-----------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cht_math_result_renamed = spark.sql(\"\"\"\n",
    "                  select \n",
    "                      age,\n",
    "                      california_region,\n",
    "                      sum(population) as s_population,\n",
    "                      avg(median_house_value) as m_median_house_value\n",
    "                  from cht_math_renamed_in_memory\n",
    "                  group by 1,2\n",
    "                  order by m_median_house_value desc\n",
    "                  \"\"\")\n",
    "\n",
    "cht_math_result_renamed.printSchema()\n",
    "\n",
    "cht_math_result_renamed.write.mode(\"overwrite\").parquet('./export2.parquet')\n",
    "\n",
    "spark.read.parquet('./export2.parquet').show()\n",
    "\n",
    "\n",
    "# * Age\n",
    "# * California_region\n",
    "# * S_population: Soma de population\n",
    "# * M_median_house_value: MÃ©dia de median_house_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf26e7-9fab-478f-8442-0525a2b378fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
